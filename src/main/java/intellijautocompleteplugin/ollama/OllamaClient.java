package intellijautocompleteplugin.ollama;

import io.github.ollama4j.OllamaAPI;
import io.github.ollama4j.exceptions.OllamaBaseException;
import io.github.ollama4j.models.OllamaAsyncResultStreamer;
import io.github.ollama4j.types.OllamaModelType;
import io.github.ollama4j.utils.OptionsBuilder;

/**
 * This class provides an interface to communicate with the Ollama API.
 * It uses the Ollama model to send queries asynchronously and retrieves responses.
 */
public class OllamaClient {
    private final OllamaAPI ollamaAPI;
    private final String MODEL = OllamaModelType.GEMMA2;

    public OllamaClient() {
        String host = "http://localhost:11434/";
        ollamaAPI = new OllamaAPI(host);
        ollamaAPI.setRequestTimeoutSeconds(60);
    }

    /**
     * Sends a query to the Ollama model asynchronously and retrieves the result.
     *
     * @param query The input query string to be processed by the model.
     * @return A String containing the response generated by the model.
     * @throws InterruptedException If the polling thread is interrupted.
     * @throws OllamaBaseException If there is an error while communicating with the Ollama API.
     */
    public String sendQuery(String query) throws InterruptedException, OllamaBaseException {
        OllamaAsyncResultStreamer streamer = ollamaAPI.generateAsync(MODEL, query, false);

        int pollIntervalMilliseconds = 1000;

        StringBuilder completeResponse = new StringBuilder();

        while (true) {
            String tokens = streamer.getStream().poll();
            if (tokens != null) {
                completeResponse.append(tokens);
            }

            if (!streamer.isAlive()) {
                break;
            }

            Thread.sleep(pollIntervalMilliseconds);
        }

        return completeResponse.toString();
    }
}
